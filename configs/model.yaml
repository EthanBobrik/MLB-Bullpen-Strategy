# -----------------------------
# Which RL algorithm are we using?
#   - "q_learning"
#   - "dqn"
#   - "cql"
# -----------------------------
model_type: "dqn"

# -----------------------------
# Action space
# -----------------------------
num_actions: 11        # 1 stay + 10 relievers

# -----------------------------
# Feature dims
# (auto-detected from dataset; override only if needed)
# -----------------------------
estimate_feature_dims: true

# -----------------------------
# DQN
# -----------------------------
dqn:
  # Architecture (if omitted, fall back to shared defaults above)
  hidden_size: 258
  num_layers: 3
  dropout: 0.10

  #early stopping
  early_stopping_patience: 10
  early_stopping_min_delta: 0.01

  # Training hyperparameters
  gamma: 0.99
  lr: 0.001
  batch_size: 512

  # Optimization schedule
  max_steps: 50000
  target_update_interval: 1000
  log_interval: 1000

  # Validation split (for TD error monitoring)
  val_fraction: 0.2

# -----------------------------
# Tabular Q-learning config
# -----------------------------
q_learning:
  gamma: 0.99
  alpha: 0.1             # learning rate for tabular updates
  epsilon_start: 0.1
  epsilon_end: 0.01

# -----------------------------
# CQL config 
# -----------------------------
cql:
  # Architecture (if omitted, fall back to shared defaults above)
  hidden_size: 64
  num_layers: 3
  dropout: 0.05

  #early stopping
  early_stopping_patience: 10
  early_stopping_min_delta: 0.01

  # Training hyperparameters
  gamma: 0.97
  alpha: 1.0
  lr: 0.0003
  batch_size: 256

  # Optimization schedule
  max_steps: 25000
  target_update_interval: 1000
  log_interval: 1000

  # Validation split (for TD error monitoring)
  val_fraction: 0.2