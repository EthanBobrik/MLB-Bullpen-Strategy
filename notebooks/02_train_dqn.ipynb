{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce5da68",
   "metadata": {},
   "source": [
    "# RL DQN Training\n",
    "\n",
    "This notebook:\n",
    "1. Loads offline RL tensors from `data/processed/rl_tensors_*.npz`\n",
    "2. Loads DQN hyperparameters from `configs/model.yaml`\n",
    "3. Trains a dueling DQN using `src/rl/dqn.py::train_dqn`\n",
    "4. Saves the trained model to `models/`\n",
    "5. Runs offline evaluation using `src/ope/offline_eval.py`:\n",
    "    - Mean Squared TD Error (MSTE)\n",
    "    - Direct Q-based value estimate of greedy policy\n",
    "    - Action agreement with logged (behavior) policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c244b60",
   "metadata": {},
   "source": [
    "## 1. Imports & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4509a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713420d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.dqn import (\n",
    "    load_dqn_training_config,\n",
    "    train_dqn,\n",
    "    BullpenOfflineDataset,\n",
    "    RLDatasetConfig,\n",
    ")\n",
    "from src.ope.offline_eval import (\n",
    "    OfflineEvalConfig,\n",
    "    load_model_and_dataset,\n",
    "    evaluate_td_error_full_mse,\n",
    "    direct_policy_value_estimate,\n",
    "    compute_policy_behavior_stats,\n",
    "    compute_q_distributions,\n",
    "    summarize_policy_behavior_stats,\n",
    "    summarize_q_distributions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3e2ab",
   "metadata": {},
   "source": [
    "## 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6264431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL tensors: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz\n",
      "Model config: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/configs/model.yaml\n",
      "Model output: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/models/dqn_bullpen_2022_2023.pt\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "YEAR_TAG = \"2022_2023\"  # must match 01_build_dataset YEARS range\n",
    "RL_TENSORS_PATH = PROC_DIR / f\"rl_tensors_{YEAR_TAG}.npz\"\n",
    "MODEL_CFG_PATH = CONFIG_DIR / \"model.yaml\"\n",
    "MODEL_OUT_PATH = MODELS_DIR / f\"dqn_bullpen_{YEAR_TAG}.pt\"\n",
    "\n",
    "print(\"RL tensors:\", RL_TENSORS_PATH)\n",
    "print(\"Model config:\", MODEL_CFG_PATH)\n",
    "print(\"Model output:\", MODEL_OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98fca4",
   "metadata": {},
   "source": [
    "## 3. Load Dataset & Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66315457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQNTrainingConfig(data_path=PosixPath('/Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz'), device='cpu', batch_size=512, lr=0.001, gamma=0.99, max_steps=50000, target_update_interval=1000, log_interval=1000, val_fraction=0.2, early_stopping_patience=10, early_stopping_min_delta=0.01, hidden_size=258, num_layers=3, dropout=0.1, yaml_num_actions=11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_cfg = load_dqn_training_config(\n",
    "    model_config_path=MODEL_CFG_PATH,\n",
    "    data_path=RL_TENSORS_PATH,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65154568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 407660\n",
      "State dim: 208\n",
      "Num actions: 11\n",
      "H (next hitters window): 5\n",
      "R (max relievers per team): 10\n"
     ]
    }
   ],
   "source": [
    "ds = BullpenOfflineDataset(\n",
    "    RLDatasetConfig(\n",
    "        data_path=train_cfg.data_path,\n",
    "        device=train_cfg.device,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Dataset size:\", len(ds))\n",
    "print(\"State dim:\", ds.state_dim)\n",
    "print(\"Num actions:\", ds.num_actions)\n",
    "print(\"H (next hitters window):\", ds.H)\n",
    "print(\"R (max relievers per team):\", ds.R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40643533",
   "metadata": {},
   "source": [
    "## 4. Create Dueling DQN Model + Trainer\n",
    "\n",
    "This calls `train_dqn(train_cfg)`, which:\n",
    " - loads `BullpenOfflineDataset` from `train_cfg.data_path`\n",
    " - splits into train/val by `train_cfg.val_fraction`\n",
    " - trains a dueling DQN with a target network\n",
    " - logs TD-error periodically using `evaluate_td_error` in `dqn.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29800c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DQN] step=0 loss=557.81396\n",
      "      val_td_error=929.83026\n",
      "      (new best val TD: 929.83026)\n",
      "[DQN] step=1000 loss=875.70972\n",
      "      val_td_error=595.31433\n",
      "      (new best val TD: 595.31433)\n",
      "[DQN] step=2000 loss=634.22729\n",
      "      val_td_error=398.42147\n",
      "      (new best val TD: 398.42147)\n",
      "[DQN] step=3000 loss=358.03110\n",
      "      val_td_error=236.46870\n",
      "      (new best val TD: 236.46870)\n",
      "[DQN] step=4000 loss=122.72629\n",
      "      val_td_error=137.71283\n",
      "      (new best val TD: 137.71283)\n",
      "[DQN] step=5000 loss=156.93430\n",
      "      val_td_error=81.05525\n",
      "      (new best val TD: 81.05525)\n",
      "[DQN] step=6000 loss=74.21658\n",
      "      val_td_error=66.47167\n",
      "      (new best val TD: 66.47167)\n",
      "[DQN] step=7000 loss=80.28516\n",
      "      val_td_error=34.00092\n",
      "      (new best val TD: 34.00092)\n",
      "[DQN] step=8000 loss=41.26678\n",
      "      val_td_error=23.20855\n",
      "      (new best val TD: 23.20855)\n",
      "[DQN] step=9000 loss=33.77260\n",
      "      val_td_error=16.76977\n",
      "      (new best val TD: 16.76977)\n",
      "[DQN] step=10000 loss=23.40019\n",
      "      val_td_error=11.11499\n",
      "      (new best val TD: 11.11499)\n",
      "[DQN] step=11000 loss=14.25980\n",
      "      val_td_error=9.15214\n",
      "      (new best val TD: 9.15214)\n",
      "[DQN] step=12000 loss=7.68210\n",
      "      val_td_error=6.41361\n",
      "      (new best val TD: 6.41361)\n",
      "[DQN] step=13000 loss=11.54392\n",
      "      val_td_error=4.93544\n",
      "      (new best val TD: 4.93544)\n",
      "[DQN] step=14000 loss=5.55960\n",
      "      val_td_error=4.04717\n",
      "      (new best val TD: 4.04717)\n",
      "[DQN] step=15000 loss=4.28998\n",
      "      val_td_error=3.26554\n",
      "      (new best val TD: 3.26554)\n",
      "[DQN] step=16000 loss=3.64018\n",
      "      val_td_error=3.04530\n",
      "      (new best val TD: 3.04530)\n",
      "[DQN] step=17000 loss=4.13638\n",
      "      val_td_error=2.06129\n",
      "      (new best val TD: 2.06129)\n",
      "[DQN] step=18000 loss=2.52531\n",
      "      val_td_error=2.02910\n",
      "      (new best val TD: 2.02910)\n",
      "[DQN] step=19000 loss=2.88739\n",
      "      val_td_error=1.80786\n",
      "      (new best val TD: 1.80786)\n",
      "[DQN] step=20000 loss=1.52070\n",
      "      val_td_error=1.61748\n",
      "      (new best val TD: 1.61748)\n",
      "[DQN] step=21000 loss=2.16707\n",
      "      val_td_error=1.31731\n",
      "      (new best val TD: 1.31731)\n",
      "[DQN] step=22000 loss=1.24808\n",
      "      val_td_error=1.41718\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=23000 loss=1.72748\n",
      "      val_td_error=1.27170\n",
      "      (new best val TD: 1.27170)\n",
      "[DQN] step=24000 loss=1.65168\n",
      "      val_td_error=1.31993\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=25000 loss=1.29011\n",
      "      val_td_error=1.10101\n",
      "      (new best val TD: 1.10101)\n",
      "[DQN] step=26000 loss=0.99183\n",
      "      val_td_error=1.16023\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=27000 loss=1.35036\n",
      "      val_td_error=1.11795\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=28000 loss=2.81456\n",
      "      val_td_error=0.99295\n",
      "      (new best val TD: 0.99295)\n",
      "[DQN] step=29000 loss=1.51451\n",
      "      val_td_error=0.97076\n",
      "      (new best val TD: 0.97076)\n",
      "[DQN] step=30000 loss=1.10355\n",
      "      val_td_error=0.98671\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=31000 loss=1.09585\n",
      "      val_td_error=1.01169\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=32000 loss=0.93990\n",
      "      val_td_error=1.09638\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=33000 loss=1.99771\n",
      "      val_td_error=1.13310\n",
      "      (no improvement for 4 val checks)\n",
      "[DQN] step=34000 loss=1.16246\n",
      "      val_td_error=0.97572\n",
      "      (no improvement for 5 val checks)\n",
      "[DQN] step=35000 loss=1.34486\n",
      "      val_td_error=0.95929\n",
      "      (new best val TD: 0.95929)\n",
      "[DQN] step=36000 loss=1.42609\n",
      "      val_td_error=1.15868\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=37000 loss=1.12886\n",
      "      val_td_error=1.22980\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=38000 loss=1.41153\n",
      "      val_td_error=1.27770\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=39000 loss=1.29941\n",
      "      val_td_error=1.21656\n",
      "      (no improvement for 4 val checks)\n",
      "[DQN] step=40000 loss=1.14658\n",
      "      val_td_error=1.34939\n",
      "      (no improvement for 5 val checks)\n",
      "[DQN] step=41000 loss=1.55285\n",
      "      val_td_error=1.41384\n",
      "      (no improvement for 6 val checks)\n",
      "[DQN] step=42000 loss=1.06927\n",
      "      val_td_error=1.32439\n",
      "      (no improvement for 7 val checks)\n",
      "[DQN] step=43000 loss=1.36429\n",
      "      val_td_error=1.21774\n",
      "      (no improvement for 8 val checks)\n",
      "[DQN] step=44000 loss=1.56629\n",
      "      val_td_error=1.24682\n",
      "      (no improvement for 9 val checks)\n",
      "[DQN] step=45000 loss=1.08985\n",
      "      val_td_error=1.43223\n",
      "      (no improvement for 10 val checks)\n",
      "[DQN] Early stopping triggered.\n",
      "[DQN] Loaded best model with val TD-error = 0.95929\n"
     ]
    }
   ],
   "source": [
    "dqn_model = train_dqn(train_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36cdf1",
   "metadata": {},
   "source": [
    "## 5. Save trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637219a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/models/dqn_bullpen_2022_2023.pt')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(dqn_model.state_dict(), MODEL_OUT_PATH)\n",
    "MODEL_OUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02761837",
   "metadata": {},
   "source": [
    "## Offline Policy Evaluation (OPE)\n",
    "\n",
    "Now we use `src/ope/offline_eval.py` to:\n",
    "- load the saved model and dataset\n",
    "- compute:\n",
    "    - Mean Squared TD Error (MSTE)\n",
    "    - Direct Q-based value of the greedy policy\n",
    "    - Action agreement with the logged policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4ca742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset size: 407660\n",
      "State dim: 208\n",
      "Num actions: 11\n"
     ]
    }
   ],
   "source": [
    "ope_cfg = OfflineEvalConfig(\n",
    "    model_config_path=MODEL_CFG_PATH,\n",
    "    model_path=MODEL_OUT_PATH,\n",
    "    tensors_path=RL_TENSORS_PATH,\n",
    "    device=device,\n",
    "    batch_size=2048,\n",
    "    gamma=train_cfg.gamma,\n",
    ")\n",
    "\n",
    "eval_model, eval_ds, eval_loader = load_model_and_dataset(ope_cfg)\n",
    "\n",
    "print(\"Eval dataset size:\", len(eval_ds))\n",
    "print(\"State dim:\", eval_ds.state_dim)\n",
    "print(\"Num actions:\", eval_ds.num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8649e3",
   "metadata": {},
   "source": [
    "## 6. Mean Squared TD Error (MSTE)\n",
    "\n",
    "This is the mean squared Bellman residual over the full dataset.\n",
    "It reuses `evaluate_td_error` from `dqn.py` under the hood,\n",
    "passing `model` as both the online and target networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0776f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared TD Error (MSTE): 0.961571\n"
     ]
    }
   ],
   "source": [
    "mste = evaluate_td_error_full_mse(\n",
    "    model=eval_model,\n",
    "    loader=eval_loader,\n",
    "    gamma=ope_cfg.gamma,\n",
    "    device=ope_cfg.device,\n",
    ")\n",
    "\n",
    "print(f\"Mean Squared TD Error (MSTE): {mste:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07deaf5",
   "metadata": {},
   "source": [
    "## 7. Direct Q-based value estimate (FQE-style Direct Method)\n",
    "\n",
    "For each state `s`:\n",
    "    - compute Q(s, a) for all actions\n",
    "    - mask unavailable actions\n",
    "    - take greedy action a* = argmax_a Q(s, a)\n",
    "    - define V_hat(s) = Q(s, a*)\n",
    "\n",
    "Then average V_hat(s) across the dataset as an estimate of V(pi_greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5295115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Q-based value estimate (V(pi_greedy)): 5.507400\n"
     ]
    }
   ],
   "source": [
    "dm_value = direct_policy_value_estimate(\n",
    "    model=eval_model,\n",
    "    loader=eval_loader,\n",
    "    device=ope_cfg.device,\n",
    ")\n",
    "\n",
    "print(f\"Direct Q-based value estimate (V(pi_greedy)): {dm_value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3281ae",
   "metadata": {},
   "source": [
    "## 8. Policy Behavior Stats and Q distributions\n",
    "\n",
    "How often does the greedy DQN action (respecting availability mask) match the logged (historical) action from the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a741dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New distributional metrics\n",
    "policy_stats = compute_policy_behavior_stats(eval_model, eval_loader, device=ope_cfg.device)\n",
    "\n",
    "q_stats = compute_q_distributions(eval_model, eval_loader, device=ope_cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0184f3e0",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a5d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= FINAL DQN EVALUATION RESULTS =========\n",
      "TD Error (MSTE):              0.961571\n",
      "Direct Q-based V(pi_greedy):  5.507400\n",
      "Action agreement rate:        8.160%\n",
      "=== Policy vs Behavior Stats ===\n",
      "Num samples:   407660\n",
      "Num actions:   11\n",
      "\n",
      "Behavior pull rate: 5.90%\n",
      "Policy pull rate:   92.10%\n",
      "Action agreement:   8.14%\n",
      "\n",
      "Behavior action counts (per action index):\n",
      "[383628   4016   3546   3018   2587   2369   2204   1923   1662   1423\n",
      "   1284]\n",
      "Policy action counts (per action index):\n",
      "[ 32206  17818  18402   3646   1047  10901  24804 213138      0   6137\n",
      "  79561]\n",
      "Valid action counts (per action index):\n",
      "[407660 368480 357208 365623 361313 371378 361377 375523 364592 364655\n",
      " 373403]\n",
      "=== Q Distribution Stats ===\n",
      "q_all_valid: n=4071212, mean=5.367, std=1.123, min=-0.868, max=122.772\n",
      "q_stay: n=407660, mean=5.232, std=1.360, min=-0.868, max=81.848\n",
      "q_best_pull: n=407660, mean=5.501, std=1.233, min=3.093, max=122.772\n",
      "q_stay_minus_best_pull: n=407660, mean=-0.269, std=0.365, min=-40.924, max=2.015\n"
     ]
    }
   ],
   "source": [
    "print(\"========= FINAL DQN EVALUATION RESULTS =========\")\n",
    "print(f\"TD Error (MSTE):              {mste:.6f}\")\n",
    "print(f\"Direct Q-based V(pi_greedy):  {dm_value:.6f}\")\n",
    "print(f\"Action agreement rate:        {agreement:.3%}\")\n",
    "summarize_policy_behavior_stats(policy_stats)\n",
    "summarize_q_distributions(q_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_folded shape: (407660,)\n",
      "reward_folded mean: -0.010023725219070911 std: 0.6859701871871948 min: -7.711379528045654 max: 1.1493159532546997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "npz = np.load(Path(\"../data/processed/rl_tensors_2022_2023.npz\"))\n",
    "\n",
    "for key in [\"reward_folded\"]:\n",
    "    x = npz[key]\n",
    "    print(key, \"shape:\", x.shape)\n",
    "    print(\n",
    "        key,\n",
    "        \"mean:\", float(x.mean()),\n",
    "        \"std:\", float(x.std()),\n",
    "        \"min:\", float(x.min()),\n",
    "        \"max:\", float(x.max()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f57095",
   "metadata": {},
   "source": [
    "After tuning the model (dropout = 0.10, hidden size = 256, 3 layers, validation fraction = 0.2, no weight decay), we obtained:\n",
    "\n",
    "\n",
    "TD Error (MSTE):              0.961571\n",
    "Direct Q-based V(pi_greedy):  5.507400\n",
    "Action agreement rate:        8.160%\n",
    "\n",
    "1. TD Error (MSTE ≈ 0.96)\n",
    "\n",
    "A TD error of 0.96 corresponds to an RMSE of about 0.98. Since the reward distribution in our offline data has mean around –0.01, standard deviation around 0.69, and values ranging from –7.7 to +1.15, this TD error indicates that the Q-network is fitting the Bellman targets reasonably well. It is not perfect, but it is significantly better than earlier runs where TD errors were much larger.\n",
    "\n",
    "2. Direct Q-based Value Estimate (≈ 5.51)\n",
    "\n",
    "The estimated value of the greedy policy induced by the DQN is around 5.5. This number is higher than what is realistically achievable given that the maximum folded reward per decision is about 1.15 and the SMDP horizon is short (no more than 3 plate appearances). This indicates overestimation, a common behavior in offline Q-learning caused by extrapolation error and the model evaluating actions that are rare or unseen in the logged data. However, this value is much more reasonable than prior extreme values (e.g., above 16).\n",
    "\n",
    "3. Action Agreement Rate (≈ 8.16%)\n",
    "\n",
    "This is the frequency with which the DQN chooses the same action as actual MLB managers in the dataset. An 8% agreement rate is low enough to show that the model is not simply copying historical decisions (which would be 60–90%), but high enough that the model’s actions are not completely random or pathological (1–2%). The model is learning a policy that differs from managers but is still grounded in the data.\n",
    "\n",
    "Overall Assessment\n",
    "\n",
    "These results indicate that:\n",
    "- The Q-function is relatively stable and fits the offline data well.\n",
    "- The greedy policy remains optimistic due to known limitations of offline DQN.\n",
    "- The model is producing meaningful, nontrivial decisions instead of behavior cloning.\n",
    "- The behavior aligns with well-known challenges of offline Q-learning and sets a strong baseline for comparison with more conservative methods like CQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ceb543",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
