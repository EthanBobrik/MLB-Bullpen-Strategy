{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce5da68",
   "metadata": {},
   "source": [
    "# RL DQN Training\n",
    "\n",
    "This notebook:\n",
    "1. Loads offline RL tensors from `data/processed/rl_tensors_*.npz`\n",
    "2. Loads DQN hyperparameters from `configs/model.yaml`\n",
    "3. Trains a dueling DQN using `src/rl/dqn.py::train_dqn`\n",
    "4. Saves the trained model to `models/`\n",
    "5. Runs offline evaluation using `src/ope/offline_eval.py`:\n",
    "    - Mean Squared TD Error (MSTE)\n",
    "    - Direct Q-based value estimate of greedy policy\n",
    "    - Action agreement with logged (behavior) policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c244b60",
   "metadata": {},
   "source": [
    "## 1. Imports & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4509a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from dataclasses import replace\n",
    "from collections import OrderedDict\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713420d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rl.dqn import (\n",
    "    load_dqn_training_config,\n",
    "    train_dqn,\n",
    "    BullpenOfflineDataset,\n",
    "    RLDatasetConfig,\n",
    ")\n",
    "from src.ope.offline_eval import (\n",
    "    evaluate_td_error_full_mse,\n",
    "    direct_policy_value_estimate,\n",
    "    compute_policy_behavior_stats,\n",
    "    compute_q_distributions,\n",
    "    summarize_policy_behavior_stats,\n",
    "    summarize_q_distributions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3e2ab",
   "metadata": {},
   "source": [
    "## 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6264431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL tensors: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz\n",
      "Model config: /Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/configs/model.yaml\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "YEAR_TAG = \"2022_2023\"  # must match 01_build_dataset YEARS range\n",
    "RL_TENSORS_PATH = PROC_DIR / f\"rl_tensors_{YEAR_TAG}.npz\"\n",
    "MODEL_CFG_PATH = CONFIG_DIR / \"model.yaml\"\n",
    "def outpath(model: str):\n",
    "    MODEL_OUT_PATH = MODELS_DIR / f\"{model}_dqn_bullpen_{YEAR_TAG}.pt\"\n",
    "    return MODEL_OUT_PATH\n",
    "\n",
    "print(\"RL tensors:\", RL_TENSORS_PATH)\n",
    "print(\"Model config:\", MODEL_CFG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98fca4",
   "metadata": {},
   "source": [
    "## 3. Load Dataset & Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66315457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQNTrainingConfig(data_path=PosixPath('/Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz'), device='cpu', batch_size=512, weight_decay=0.0, lr=0.001, gamma=0.99, max_steps=50000, target_update_interval=1000, log_interval=1000, val_fraction=0.2, early_stopping_patience=10, early_stopping_min_delta=0.01, hidden_size=258, num_layers=3, dropout=0.1, grad_clip_max_norm=0.0, yaml_num_actions=11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_cfg = load_dqn_training_config(\n",
    "    model_config_path=MODEL_CFG_PATH,\n",
    "    data_path=RL_TENSORS_PATH,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "train_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9b17b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DQNTrainingConfig(data_path=PosixPath('/Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz'), device='cpu', batch_size=512, weight_decay=0.0, lr=0.001, gamma=0.99, max_steps=50000, target_update_interval=1000, log_interval=1000, val_fraction=0.2, early_stopping_patience=10, early_stopping_min_delta=0.01, hidden_size=128, num_layers=2, dropout=0.05, grad_clip_max_norm=0.0, yaml_num_actions=11),\n",
       " DQNTrainingConfig(data_path=PosixPath('/Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz'), device='cpu', batch_size=512, weight_decay=0.0, lr=0.001, gamma=0.99, max_steps=50000, target_update_interval=1000, log_interval=1000, val_fraction=0.2, early_stopping_patience=10, early_stopping_min_delta=0.01, hidden_size=256, num_layers=4, dropout=0.1, grad_clip_max_norm=0.0, yaml_num_actions=11),\n",
       " DQNTrainingConfig(data_path=PosixPath('/Users/ethanbobrik/Projects/MLB-Bullpen-Strategy/data/processed/rl_tensors_2022_2023.npz'), device='cpu', batch_size=512, weight_decay=0.0001, lr=0.001, gamma=0.99, max_steps=50000, target_update_interval=1000, log_interval=1000, val_fraction=0.2, early_stopping_patience=10, early_stopping_min_delta=0.01, hidden_size=256, num_layers=4, dropout=0.1, grad_clip_max_norm=5.0, yaml_num_actions=11))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We treat train_cfg as a base config coming from configs/model.yaml\n",
    "base_cfg = train_cfg\n",
    "\n",
    "# 1) Shallow model: smaller net, less layers, light regularization\n",
    "cfg_shallow = replace(\n",
    "    base_cfg,\n",
    "    hidden_size=128,          # fewer hidden units\n",
    "    num_layers=2,             # shallower\n",
    "    dropout=0.05,             # small dropout\n",
    "    weight_decay=0.0,         # no L2\n",
    "    grad_clip_max_norm=0.0,   # no grad clipping\n",
    ")\n",
    "\n",
    "# 2) Deeper model: larger network, same regularization as base\n",
    "cfg_deep = replace(\n",
    "    base_cfg,\n",
    "    hidden_size=256,\n",
    "    num_layers=4,\n",
    "    dropout=0.10,\n",
    "    weight_decay=0.0,\n",
    "    grad_clip_max_norm=0.0,\n",
    ")\n",
    "\n",
    "# 3) Constrained model: like deep, but with weight decay + grad clipping\n",
    "cfg_constrained = replace(\n",
    "    base_cfg,\n",
    "    hidden_size=256,\n",
    "    num_layers=4,\n",
    "    dropout=0.10,\n",
    "    weight_decay=1e-4,        # L2 regularization\n",
    "    grad_clip_max_norm=5.0,   # clip gradients by global norm\n",
    ")\n",
    "\n",
    "cfg_shallow, cfg_deep, cfg_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65154568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 407660\n",
      "State dim: 208\n",
      "Num actions: 11\n",
      "H (next hitters window): 5\n",
      "R (max relievers per team): 10\n"
     ]
    }
   ],
   "source": [
    "ds = BullpenOfflineDataset(\n",
    "    RLDatasetConfig(\n",
    "        data_path=base_cfg.data_path,\n",
    "        device=base_cfg.device,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Dataset size:\", len(ds))\n",
    "print(\"State dim:\", ds.state_dim)\n",
    "print(\"Num actions:\", ds.num_actions)\n",
    "print(\"H (next hitters window):\", ds.H)\n",
    "print(\"R (max relievers per team):\", ds.R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40643533",
   "metadata": {},
   "source": [
    "## 4. Create Dueling DQN Model + Trainer\n",
    "\n",
    "This calls `train_dqn(train_cfg)`, which:\n",
    " - loads `BullpenOfflineDataset` from `train_cfg.data_path`\n",
    " - splits into train/val by `train_cfg.val_fraction`\n",
    " - trains a dueling DQN with a target network\n",
    " - logs TD-error periodically using `evaluate_td_error` in `dqn.py`\n",
    "\n",
    "This is done for 3 different models with differing complexity,\n",
    "1. Shallow DQN Model\n",
    "2. Deep DQN Model\n",
    "3. Deep DQN Model with Gradient Clipping + Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29800c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training SHALLOW DQN model ===\n",
      "[DQN] step=0 loss=14947.88672\n",
      "      val_td_error=10213.48000\n",
      "      (new best val TD: 10213.48000)\n",
      "[DQN] step=1000 loss=5245.73730\n",
      "      val_td_error=4542.58110\n",
      "      (new best val TD: 4542.58110)\n",
      "[DQN] step=2000 loss=4347.56445\n",
      "      val_td_error=3995.27182\n",
      "      (new best val TD: 3995.27182)\n",
      "[DQN] step=3000 loss=4682.87256\n",
      "      val_td_error=2884.55904\n",
      "      (new best val TD: 2884.55904)\n",
      "[DQN] step=4000 loss=2148.10547\n",
      "      val_td_error=2171.13028\n",
      "      (new best val TD: 2171.13028)\n",
      "[DQN] step=5000 loss=3191.11377\n",
      "      val_td_error=1689.13913\n",
      "      (new best val TD: 1689.13913)\n",
      "[DQN] step=6000 loss=1882.16577\n",
      "      val_td_error=1585.43341\n",
      "      (new best val TD: 1585.43341)\n",
      "[DQN] step=7000 loss=1254.97729\n",
      "      val_td_error=1283.68585\n",
      "      (new best val TD: 1283.68585)\n",
      "[DQN] step=8000 loss=1300.65356\n",
      "      val_td_error=1180.03556\n",
      "      (new best val TD: 1180.03556)\n",
      "[DQN] step=9000 loss=1202.85095\n",
      "      val_td_error=1194.82069\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=10000 loss=1110.26575\n",
      "      val_td_error=930.59845\n",
      "      (new best val TD: 930.59845)\n",
      "[DQN] step=11000 loss=1066.57788\n",
      "      val_td_error=729.90708\n",
      "      (new best val TD: 729.90708)\n",
      "[DQN] step=12000 loss=668.11383\n",
      "      val_td_error=767.21932\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=13000 loss=697.76819\n",
      "      val_td_error=774.07373\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=14000 loss=966.99249\n",
      "      val_td_error=746.97400\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=15000 loss=591.02209\n",
      "      val_td_error=534.10635\n",
      "      (new best val TD: 534.10635)\n",
      "[DQN] step=16000 loss=665.00110\n",
      "      val_td_error=515.79193\n",
      "      (new best val TD: 515.79193)\n",
      "[DQN] step=17000 loss=556.42651\n",
      "      val_td_error=432.60884\n",
      "      (new best val TD: 432.60884)\n",
      "[DQN] step=18000 loss=477.94257\n",
      "      val_td_error=370.19285\n",
      "      (new best val TD: 370.19285)\n",
      "[DQN] step=19000 loss=505.53680\n",
      "      val_td_error=358.05897\n",
      "      (new best val TD: 358.05897)\n",
      "[DQN] step=20000 loss=442.73657\n",
      "      val_td_error=313.85099\n",
      "      (new best val TD: 313.85099)\n",
      "[DQN] step=21000 loss=270.39078\n",
      "      val_td_error=384.31627\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=22000 loss=307.95929\n",
      "      val_td_error=314.88895\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=23000 loss=335.68988\n",
      "      val_td_error=279.27570\n",
      "      (new best val TD: 279.27570)\n",
      "[DQN] step=24000 loss=247.30414\n",
      "      val_td_error=255.65759\n",
      "      (new best val TD: 255.65759)\n",
      "[DQN] step=25000 loss=243.14261\n",
      "      val_td_error=256.68881\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=26000 loss=268.73550\n",
      "      val_td_error=209.65018\n",
      "      (new best val TD: 209.65018)\n",
      "[DQN] step=27000 loss=344.27725\n",
      "      val_td_error=242.06961\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=28000 loss=182.42868\n",
      "      val_td_error=194.05210\n",
      "      (new best val TD: 194.05210)\n",
      "[DQN] step=29000 loss=230.08923\n",
      "      val_td_error=179.14767\n",
      "      (new best val TD: 179.14767)\n",
      "[DQN] step=30000 loss=119.13980\n",
      "      val_td_error=170.59017\n",
      "      (new best val TD: 170.59017)\n",
      "[DQN] step=31000 loss=240.87015\n",
      "      val_td_error=137.45544\n",
      "      (new best val TD: 137.45544)\n",
      "[DQN] step=32000 loss=163.21536\n",
      "      val_td_error=124.88713\n",
      "      (new best val TD: 124.88713)\n",
      "[DQN] step=33000 loss=176.71762\n",
      "      val_td_error=125.57503\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=34000 loss=168.46304\n",
      "      val_td_error=113.39711\n",
      "      (new best val TD: 113.39711)\n",
      "[DQN] step=35000 loss=207.83295\n",
      "      val_td_error=111.25859\n",
      "      (new best val TD: 111.25859)\n",
      "[DQN] step=36000 loss=122.10563\n",
      "      val_td_error=125.68502\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=37000 loss=135.21687\n",
      "      val_td_error=126.20970\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=38000 loss=113.77493\n",
      "      val_td_error=144.39343\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=39000 loss=116.52336\n",
      "      val_td_error=102.27612\n",
      "      (new best val TD: 102.27612)\n",
      "[DQN] step=40000 loss=133.33392\n",
      "      val_td_error=106.38656\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=41000 loss=137.26407\n",
      "      val_td_error=135.96996\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=42000 loss=142.20859\n",
      "      val_td_error=114.83512\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=43000 loss=164.58566\n",
      "      val_td_error=96.77808\n",
      "      (new best val TD: 96.77808)\n",
      "[DQN] step=44000 loss=95.58644\n",
      "      val_td_error=88.73682\n",
      "      (new best val TD: 88.73682)\n",
      "[DQN] step=45000 loss=76.79191\n",
      "      val_td_error=85.48518\n",
      "      (new best val TD: 85.48518)\n",
      "[DQN] step=46000 loss=119.64436\n",
      "      val_td_error=72.96963\n",
      "      (new best val TD: 72.96963)\n",
      "[DQN] step=47000 loss=58.87691\n",
      "      val_td_error=90.83719\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=48000 loss=70.45495\n",
      "      val_td_error=70.02118\n",
      "      (new best val TD: 70.02118)\n",
      "[DQN] step=49000 loss=62.93246\n",
      "      val_td_error=63.22992\n",
      "      (new best val TD: 63.22992)\n",
      "[DQN] Loaded best model with val TD-error = 63.22992\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Training SHALLOW DQN model ===\")\n",
    "shallow_dqn = train_dqn(cfg_shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b02bd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training DEEP DQN model ===\n",
      "[DQN] step=0 loss=280.62946\n",
      "      val_td_error=478.21731\n",
      "      (new best val TD: 478.21731)\n",
      "[DQN] step=1000 loss=298.60492\n",
      "      val_td_error=255.84395\n",
      "      (new best val TD: 255.84395)\n",
      "[DQN] step=2000 loss=266.53717\n",
      "      val_td_error=135.70832\n",
      "      (new best val TD: 135.70832)\n",
      "[DQN] step=3000 loss=177.20477\n",
      "      val_td_error=79.65328\n",
      "      (new best val TD: 79.65328)\n",
      "[DQN] step=4000 loss=75.76599\n",
      "      val_td_error=39.50763\n",
      "      (new best val TD: 39.50763)\n",
      "[DQN] step=5000 loss=34.35710\n",
      "      val_td_error=22.11051\n",
      "      (new best val TD: 22.11051)\n",
      "[DQN] step=6000 loss=26.93233\n",
      "      val_td_error=11.57459\n",
      "      (new best val TD: 11.57459)\n",
      "[DQN] step=7000 loss=13.69288\n",
      "      val_td_error=7.12578\n",
      "      (new best val TD: 7.12578)\n",
      "[DQN] step=8000 loss=6.37473\n",
      "      val_td_error=4.75054\n",
      "      (new best val TD: 4.75054)\n",
      "[DQN] step=9000 loss=4.63814\n",
      "      val_td_error=3.13497\n",
      "      (new best val TD: 3.13497)\n",
      "[DQN] step=10000 loss=3.63013\n",
      "      val_td_error=2.49900\n",
      "      (new best val TD: 2.49900)\n",
      "[DQN] step=11000 loss=2.14045\n",
      "      val_td_error=1.75453\n",
      "      (new best val TD: 1.75453)\n",
      "[DQN] step=12000 loss=1.52786\n",
      "      val_td_error=1.57765\n",
      "      (new best val TD: 1.57765)\n",
      "[DQN] step=13000 loss=1.30226\n",
      "      val_td_error=1.46949\n",
      "      (new best val TD: 1.46949)\n",
      "[DQN] step=14000 loss=1.59606\n",
      "      val_td_error=1.25935\n",
      "      (new best val TD: 1.25935)\n",
      "[DQN] step=15000 loss=1.66204\n",
      "      val_td_error=1.33834\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=16000 loss=1.27368\n",
      "      val_td_error=1.15172\n",
      "      (new best val TD: 1.15172)\n",
      "[DQN] step=17000 loss=1.24612\n",
      "      val_td_error=1.10315\n",
      "      (new best val TD: 1.10315)\n",
      "[DQN] step=18000 loss=0.94192\n",
      "      val_td_error=1.19900\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=19000 loss=1.06968\n",
      "      val_td_error=1.19101\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=20000 loss=1.35844\n",
      "      val_td_error=1.25238\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=21000 loss=1.18951\n",
      "      val_td_error=1.28441\n",
      "      (no improvement for 4 val checks)\n",
      "[DQN] step=22000 loss=1.65066\n",
      "      val_td_error=1.49317\n",
      "      (no improvement for 5 val checks)\n",
      "[DQN] step=23000 loss=1.21592\n",
      "      val_td_error=1.43166\n",
      "      (no improvement for 6 val checks)\n",
      "[DQN] step=24000 loss=1.30160\n",
      "      val_td_error=1.26431\n",
      "      (no improvement for 7 val checks)\n",
      "[DQN] step=25000 loss=1.34326\n",
      "      val_td_error=1.24444\n",
      "      (no improvement for 8 val checks)\n",
      "[DQN] step=26000 loss=1.47283\n",
      "      val_td_error=1.33987\n",
      "      (no improvement for 9 val checks)\n",
      "[DQN] step=27000 loss=1.44957\n",
      "      val_td_error=1.31376\n",
      "      (no improvement for 10 val checks)\n",
      "[DQN] Early stopping triggered.\n",
      "[DQN] Loaded best model with val TD-error = 1.10315\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Training DEEP DQN model ===\")\n",
    "deep_dqn = train_dqn(cfg_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5856b923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training CONSTRAINED DQN model (weight decay + grad clipping) ===\n",
      "[DQN] step=0 loss=107.75221\n",
      "      val_td_error=454.41000\n",
      "      (new best val TD: 454.41000)\n",
      "[DQN] step=1000 loss=389.82013\n",
      "      val_td_error=178.24252\n",
      "      (new best val TD: 178.24252)\n",
      "[DQN] step=2000 loss=238.07759\n",
      "      val_td_error=109.96997\n",
      "      (new best val TD: 109.96997)\n",
      "[DQN] step=3000 loss=109.37428\n",
      "      val_td_error=57.40383\n",
      "      (new best val TD: 57.40383)\n",
      "[DQN] step=4000 loss=49.18357\n",
      "      val_td_error=34.61348\n",
      "      (new best val TD: 34.61348)\n",
      "[DQN] step=5000 loss=30.85622\n",
      "      val_td_error=22.31415\n",
      "      (new best val TD: 22.31415)\n",
      "[DQN] step=6000 loss=21.81225\n",
      "      val_td_error=9.68241\n",
      "      (new best val TD: 9.68241)\n",
      "[DQN] step=7000 loss=13.36385\n",
      "      val_td_error=5.45047\n",
      "      (new best val TD: 5.45047)\n",
      "[DQN] step=8000 loss=5.71718\n",
      "      val_td_error=3.24512\n",
      "      (new best val TD: 3.24512)\n",
      "[DQN] step=9000 loss=5.75646\n",
      "      val_td_error=2.68200\n",
      "      (new best val TD: 2.68200)\n",
      "[DQN] step=10000 loss=3.45722\n",
      "      val_td_error=2.06951\n",
      "      (new best val TD: 2.06951)\n",
      "[DQN] step=11000 loss=2.18162\n",
      "      val_td_error=1.62359\n",
      "      (new best val TD: 1.62359)\n",
      "[DQN] step=12000 loss=2.43348\n",
      "      val_td_error=1.54610\n",
      "      (new best val TD: 1.54610)\n",
      "[DQN] step=13000 loss=1.79542\n",
      "      val_td_error=1.34906\n",
      "      (new best val TD: 1.34906)\n",
      "[DQN] step=14000 loss=1.52793\n",
      "      val_td_error=1.13570\n",
      "      (new best val TD: 1.13570)\n",
      "[DQN] step=15000 loss=1.34398\n",
      "      val_td_error=1.09162\n",
      "      (new best val TD: 1.09162)\n",
      "[DQN] step=16000 loss=1.21216\n",
      "      val_td_error=1.07731\n",
      "      (new best val TD: 1.07731)\n",
      "[DQN] step=17000 loss=1.02436\n",
      "      val_td_error=1.12084\n",
      "      (no improvement for 1 val checks)\n",
      "[DQN] step=18000 loss=1.14962\n",
      "      val_td_error=1.12429\n",
      "      (no improvement for 2 val checks)\n",
      "[DQN] step=19000 loss=1.30558\n",
      "      val_td_error=1.30004\n",
      "      (no improvement for 3 val checks)\n",
      "[DQN] step=20000 loss=1.39200\n",
      "      val_td_error=1.20562\n",
      "      (no improvement for 4 val checks)\n",
      "[DQN] step=21000 loss=1.40582\n",
      "      val_td_error=1.22428\n",
      "      (no improvement for 5 val checks)\n",
      "[DQN] step=22000 loss=1.41629\n",
      "      val_td_error=1.53540\n",
      "      (no improvement for 6 val checks)\n",
      "[DQN] step=23000 loss=1.02655\n",
      "      val_td_error=1.42743\n",
      "      (no improvement for 7 val checks)\n",
      "[DQN] step=24000 loss=1.77940\n",
      "      val_td_error=1.36693\n",
      "      (no improvement for 8 val checks)\n",
      "[DQN] step=25000 loss=1.55705\n",
      "      val_td_error=1.31073\n",
      "      (no improvement for 9 val checks)\n",
      "[DQN] step=26000 loss=1.26468\n",
      "      val_td_error=1.41179\n",
      "      (no improvement for 10 val checks)\n",
      "[DQN] Early stopping triggered.\n",
      "[DQN] Loaded best model with val TD-error = 1.07731\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Training CONSTRAINED DQN model (weight decay + grad clipping) ===\")\n",
    "constrained_dqn = train_dqn(cfg_constrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36cdf1",
   "metadata": {},
   "source": [
    "## 5. Save trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637219a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_outpath = outpath('shallow')\n",
    "deep_outpath = outpath('deep')\n",
    "constrained_outpath = outpath('constrained')\n",
    "\n",
    "\n",
    "torch.save(shallow_dqn.state_dict(), shallow_outpath)\n",
    "torch.save(deep_dqn.state_dict(), deep_outpath)\n",
    "torch.save(constrained_dqn.state_dict(), constrained_outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02761837",
   "metadata": {},
   "source": [
    "## Offline Policy Evaluation (OPE)\n",
    "\n",
    "Now we use `src/ope/offline_eval.py` to:\n",
    "- load the saved model and dataset\n",
    "- compute:\n",
    "    - Mean Squared TD Error (MSTE)\n",
    "    - Direct Q-based value of the greedy policy\n",
    "    - Action agreement with the logged policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4ca742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset size: 407660\n",
      "State dim: 208\n",
      "Num actions: 11\n"
     ]
    }
   ],
   "source": [
    "eval_ds = BullpenOfflineDataset(\n",
    "    RLDatasetConfig(\n",
    "        data_path=RL_TENSORS_PATH,\n",
    "        device=device,\n",
    "    )\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(eval_ds, batch_size=2048, shuffle=False)\n",
    "\n",
    "print(\"Eval dataset size:\", len(eval_ds))\n",
    "print(\"State dim:\", eval_ds.state_dim)\n",
    "print(\"Num actions:\", eval_ds.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d372bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== SHALLOW MODEL ====================\n",
      "TD Error (MSTE):              50.840302\n",
      "Direct Q-based V(pi_greedy):  65.003840\n",
      "Action agreement rate:        30.793%\n",
      "Behavior pull rate:           5.895%\n",
      "Policy pull rate:             67.621%\n",
      "\n",
      "[Policy vs Behavior stats]\n",
      "=== Policy vs Behavior Stats ===\n",
      "Num samples:   407660\n",
      "Num actions:   11\n",
      "\n",
      "Behavior pull rate: 5.90%\n",
      "Policy pull rate:   67.62%\n",
      "Action agreement:   30.79%\n",
      "\n",
      "Behavior action counts (per action index):\n",
      "[383628   4016   3546   3018   2587   2369   2204   1923   1662   1423\n",
      "   1284]\n",
      "Policy action counts (per action index):\n",
      "[131997  64027   2027   8960  70358    340  25648  52188   5460  29572\n",
      "  17083]\n",
      "Valid action counts (per action index):\n",
      "[407660 368480 357208 365623 361313 371378 361377 375523 364592 364655\n",
      " 373403]\n",
      "\n",
      "[Q-value distribution stats]\n",
      "=== Q Distribution Stats ===\n",
      "q_all_valid: n=4071212, mean=63.769, std=47.255, min=-33.747, max=1537.623\n",
      "q_stay: n=407660, mean=62.601, std=47.499, min=-33.747, max=1476.034\n",
      "q_best_pull: n=407660, mean=64.708, std=49.733, min=1.137, max=1537.623\n",
      "q_stay_minus_best_pull: n=407660, mean=-2.106, std=5.078, min=-240.200, max=14.085\n",
      "======================================================\n",
      "\n",
      "==================== DEEP MODEL ====================\n",
      "TD Error (MSTE):              1.080123\n",
      "Direct Q-based V(pi_greedy):  5.475471\n",
      "Action agreement rate:        6.522%\n",
      "Behavior pull rate:           5.895%\n",
      "Policy pull rate:             94.349%\n",
      "\n",
      "[Policy vs Behavior stats]\n",
      "=== Policy vs Behavior Stats ===\n",
      "Num samples:   407660\n",
      "Num actions:   11\n",
      "\n",
      "Behavior pull rate: 5.90%\n",
      "Policy pull rate:   94.35%\n",
      "Action agreement:   6.52%\n",
      "\n",
      "Behavior action counts (per action index):\n",
      "[383628   4016   3546   3018   2587   2369   2204   1923   1662   1423\n",
      "   1284]\n",
      "Policy action counts (per action index):\n",
      "[ 23036  81390 264077   4627   2238   5607  15112   1727   4558    164\n",
      "   5124]\n",
      "Valid action counts (per action index):\n",
      "[407660 368480 357208 365623 361313 371378 361377 375523 364592 364655\n",
      " 373403]\n",
      "\n",
      "[Q-value distribution stats]\n",
      "=== Q Distribution Stats ===\n",
      "q_all_valid: n=4071212, mean=5.398, std=0.522, min=0.311, max=52.907\n",
      "q_stay: n=407660, mean=5.206, std=0.761, min=0.311, max=41.585\n",
      "q_best_pull: n=407660, mean=5.475, std=0.489, min=1.373, max=52.907\n",
      "q_stay_minus_best_pull: n=407660, mean=-0.269, std=0.401, min=-13.153, max=0.723\n",
      "======================================================\n",
      "\n",
      "==================== CONSTRAINED MODEL ====================\n",
      "TD Error (MSTE):              1.087532\n",
      "Direct Q-based V(pi_greedy):  4.787750\n",
      "Action agreement rate:        3.073%\n",
      "Behavior pull rate:           5.895%\n",
      "Policy pull rate:             97.593%\n",
      "\n",
      "[Policy vs Behavior stats]\n",
      "=== Policy vs Behavior Stats ===\n",
      "Num samples:   407660\n",
      "Num actions:   11\n",
      "\n",
      "Behavior pull rate: 5.90%\n",
      "Policy pull rate:   97.59%\n",
      "Action agreement:   3.07%\n",
      "\n",
      "Behavior action counts (per action index):\n",
      "[383628   4016   3546   3018   2587   2369   2204   1923   1662   1423\n",
      "   1284]\n",
      "Policy action counts (per action index):\n",
      "[  9814 121866  28311    492      2  16154    175     16    236 230012\n",
      "    582]\n",
      "Valid action counts (per action index):\n",
      "[407660 368480 357208 365623 361313 371378 361377 375523 364592 364655\n",
      " 373403]\n",
      "\n",
      "[Q-value distribution stats]\n",
      "=== Q Distribution Stats ===\n",
      "q_all_valid: n=4071212, mean=4.710, std=0.328, min=2.490, max=21.585\n",
      "q_stay: n=407660, mean=4.523, std=0.526, min=2.490, max=17.428\n",
      "q_best_pull: n=407660, mean=4.787, std=0.291, min=3.879, max=21.585\n",
      "q_stay_minus_best_pull: n=407660, mean=-0.264, std=0.304, min=-4.157, max=0.106\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Ensure models are on correct device\n",
    "dqn_shallow = shallow_dqn.to(device)\n",
    "dqn_deep = deep_dqn.to(device)\n",
    "dqn_constrained = constrained_dqn.to(device)\n",
    "\n",
    "gamma = cfg_constrained.gamma\n",
    "\n",
    "models_to_eval = OrderedDict(\n",
    "    [\n",
    "        (\"shallow\", dqn_shallow),\n",
    "        (\"deep\", dqn_deep),\n",
    "        (\"constrained\", dqn_constrained),\n",
    "    ]\n",
    ")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for name, model in models_to_eval.items():\n",
    "    print(f\"\\n==================== {name.upper()} MODEL ====================\")\n",
    "    model.eval()\n",
    "\n",
    "    # 1) TD Error (MSTE)\n",
    "    mste_i = evaluate_td_error_full_mse(\n",
    "        model=model,\n",
    "        loader=eval_loader,\n",
    "        gamma=gamma,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 2) Direct Q-based value estimate\n",
    "    dm_value_i = direct_policy_value_estimate(\n",
    "        model=model,\n",
    "        loader=eval_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 3) Policy vs behavior stats\n",
    "    policy_stats_i = compute_policy_behavior_stats(\n",
    "        model=model,\n",
    "        loader=eval_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 4) Q-value distribution stats\n",
    "    q_stats_i = compute_q_distributions(\n",
    "        model=model,\n",
    "        loader=eval_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    all_results[name] = {\n",
    "        \"mste\": mste_i,\n",
    "        \"dm_value\": dm_value_i,\n",
    "        \"policy_stats\": policy_stats_i,\n",
    "        \"q_stats\": q_stats_i,\n",
    "    }\n",
    "\n",
    "    agreement_rate_i = policy_stats_i[\"agreement_rate\"]\n",
    "    behavior_pull_rate_i = policy_stats_i[\"behavior_pull_rate\"]\n",
    "    policy_pull_rate_i = policy_stats_i[\"policy_pull_rate\"]\n",
    "\n",
    "    print(f\"TD Error (MSTE):              {mste_i:.6f}\")\n",
    "    print(f\"Direct Q-based V(pi_greedy):  {dm_value_i:.6f}\")\n",
    "    print(f\"Action agreement rate:        {agreement_rate_i*100:.3f}%\")\n",
    "    print(f\"Behavior pull rate:           {behavior_pull_rate_i*100:.3f}%\")\n",
    "    print(f\"Policy pull rate:             {policy_pull_rate_i*100:.3f}%\")\n",
    "\n",
    "    print(\"\\n[Policy vs Behavior stats]\")\n",
    "    summarize_policy_behavior_stats(policy_stats_i)\n",
    "\n",
    "    print(\"\\n[Q-value distribution stats]\")\n",
    "    summarize_q_distributions(q_stats_i)\n",
    "    print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f9b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_folded shape: (407660,)\n",
      "reward_folded mean: -0.010023725219070911 std: 0.6859701871871948 min: -7.711379528045654 max: 1.1493159532546997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "npz = np.load(Path(\"../data/processed/rl_tensors_2022_2023.npz\"))\n",
    "\n",
    "for key in [\"reward_folded\"]:\n",
    "    x = npz[key]\n",
    "    print(key, \"shape:\", x.shape)\n",
    "    print(\n",
    "        key,\n",
    "        \"mean:\", float(x.mean()),\n",
    "        \"std:\", float(x.std()),\n",
    "        \"min:\", float(x.min()),\n",
    "        \"max:\", float(x.max()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f57095",
   "metadata": {},
   "source": [
    "After tuning the model (dropout = 0.10, hidden size = 256, 3 layers, validation fraction = 0.2, no weight decay), we obtained:\n",
    "\n",
    "\n",
    "TD Error (MSTE):              0.961571\n",
    "Direct Q-based V(pi_greedy):  5.507400\n",
    "Action agreement rate:        8.160%\n",
    "\n",
    "1. TD Error (MSTE ≈ 0.96)\n",
    "\n",
    "A TD error of 0.96 corresponds to an RMSE of about 0.98. Since the reward distribution in our offline data has mean around –0.01, standard deviation around 0.69, and values ranging from –7.7 to +1.15, this TD error indicates that the Q-network is fitting the Bellman targets reasonably well. It is not perfect, but it is significantly better than earlier runs where TD errors were much larger.\n",
    "\n",
    "2. Direct Q-based Value Estimate (≈ 5.51)\n",
    "\n",
    "The estimated value of the greedy policy induced by the DQN is around 5.5. This number is higher than what is realistically achievable given that the maximum folded reward per decision is about 1.15 and the SMDP horizon is short (no more than 3 plate appearances). This indicates overestimation, a common behavior in offline Q-learning caused by extrapolation error and the model evaluating actions that are rare or unseen in the logged data. However, this value is much more reasonable than prior extreme values (e.g., above 16).\n",
    "\n",
    "3. Action Agreement Rate (≈ 8.16%)\n",
    "\n",
    "This is the frequency with which the DQN chooses the same action as actual MLB managers in the dataset. An 8% agreement rate is low enough to show that the model is not simply copying historical decisions (which would be 60–90%), but high enough that the model’s actions are not completely random or pathological (1–2%). The model is learning a policy that differs from managers but is still grounded in the data.\n",
    "\n",
    "Overall Assessment\n",
    "\n",
    "These results indicate that:\n",
    "- The Q-function is relatively stable and fits the offline data well.\n",
    "- The greedy policy remains optimistic due to known limitations of offline DQN.\n",
    "- The model is producing meaningful, nontrivial decisions instead of behavior cloning.\n",
    "- The behavior aligns with well-known challenges of offline Q-learning and sets a strong baseline for comparison with more conservative methods like CQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ceb543",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
